import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
#from main import _calculate_fan_in_and_fan_out

class Mlp(nn.Module):
    def __init__(self, network_depth, in_features, hidden_features=None, out_features=None):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.network_depth = network_depth

        self.mlp = nn.Sequential(
            nn.Conv2d(in_features, hidden_features, 1),
            nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features),
            nn.ReLU(True),
            nn.Conv2d(hidden_features, out_features, 1)
        )

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Conv2d):
            gain = (8 * self.network_depth) ** (-1 / 4)
            fan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)
            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))
            trunc_normal_(m.weight, std=std)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.mlp(x)


class CEFN011(nn.Module):
    def __init__(self, dim, network_depth, hidden_features=None, out_features=None):
        super(CEFN011, self).__init__()
        
        
        self.mlp = Mlp(network_depth=network_depth, in_features=dim, hidden_features=hidden_features,
                       out_features=out_features)
        self.norm = LayerNorm(dim, eps=1e-5)
        
        self.AVG = nn.AdaptiveAvgPool2d(1)
        
        self.mixConv2 = MixConv(dim,[3,5,1],1)
        self.Linear2 = nn.Conv2d(dim, dim,1, stride=1, padding=0, groups=dim, dilation=1, padding_mode='reflect')
        self.Conv32 = nn.Conv2d(dim, dim, 3, stride=1, padding=1, groups=dim, dilation=1, padding_mode='reflect')
        self.Conv52 = nn.Conv2d(dim, dim, 5, stride=1, padding=2, groups=dim, dilation=1, padding_mode='reflect')
        self.mixConv3 = MixConv(dim,[3,5,1],1)
        self.Linear3 = nn.Conv2d(dim+dim+dim, dim,1, stride=1, padding=0, groups=dim, dilation=1, padding_mode='reflect')
        self.Conv33 = nn.Conv2d(dim+dim+dim, dim, 3, stride=1, padding=1, groups=dim, dilation=1, padding_mode='reflect')
        self.Conv53 = nn.Conv2d(dim+dim+dim, dim, 5, stride=1, padding=2, groups=dim, dilation=1, padding_mode='reflect')
        self.sig = nn.Sigmoid()
        
        self.scaler = nn.Parameter(torch.ones(dim, 1, 1))

    def forward(self, x):
        
        x1 = self.mixConv2(x)
        
        x11 = self.Conv32(x)
        x12 = self.Conv52(x)
        C1 = torch.concatenate((x1, x11,x12), dim=1)
       # C1 = x1*x11*x12
        #C1 = x1+x11+x12
        x2 = self.mixConv3(C1)
        x2ii = self.Linear2(x2)
        x21 = self.Conv33(C1)
        x22 = self.Conv53(C1)
        C2 =torch.concatenate((x2ii, x21,x22), dim=1)
       # C2 = x2ii* x21*x22
       # C2 = x2ii+ x21+x22
        xx = self.Linear3(C2)
        xx1 = self.sig(xx)
        attn = self.scaler* xx1
        xv = self.norm(self.mlp(x))
        
        return xv* attn
    
MM18 = CEFN011(3,2)(y)
print(MM18.shape)
